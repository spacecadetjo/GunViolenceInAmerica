{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCFemTech Hack for Good: Gun Violence in the United States\n",
    "\n",
    "#### What is DCFemTech?\n",
    "DCFemTech is a coalition of women leaders aimed at amplifying the efforts of women in tech organizations, sharing resources, and bringing leaders together to close the gender gap.\n",
    "\n",
    "#### What is Hack for Good?\n",
    "Hack for Good is a 2-day event bringing folks together to work on projects that support our community. Progress on projects will be presented in a science fair format at the conclusion of the event. This is a different kind of hackathon. No competition, no award money; just bringing people together to learn, make connections, and build something awesome!\n",
    "\n",
    "### About the Challenge and the Dataset\n",
    "\n",
    "##### BACKGROUND \n",
    "Gun violence in the United States has been steadily rising in the last decade. A recent study published in the American Journal of Medicine found that Americans are 25 times more likely to die from gun violence than people in other developed countries. \n",
    "\n",
    "##### DATASET \n",
    "The dataset chosen for this challenge is a comprehensive record of over 260,000 U.S. gun violence incidents between January 2013 and March 2018. \n",
    "\n",
    "* https://www.kaggle.com/jameslko/gun-violence-data\n",
    "\n",
    "* https://github.com/spacecadetjo/GunViolenceInAmerica/tree/master/Data\n",
    "\n",
    "##### THE PROBLEM \n",
    "With the most complete record of gun violence in America at our fingertips, what information can we glean about this epidemic? Using a combination of geospatial and time series analysis, can we draw meaningful conclusions that can inform public health and government policies? Working together with facilitators from Booz Allen Hamilton's Data Science team, this notebook presents the findings of new data scientists of various skill levels on the gun violence epidemic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "First we need to import the packages we'll be using during the exercise. This has already been done for you. Just hit run on the cell to execute the package installation. Remember, if you're using anaconda, you'll need to create a new kernel (follow the instructions in the participant's guide)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "plotly.offline.init_notebook_mode()\n",
    "from plotly.offline import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have packages, you'll need to read in the data from the CSVs. If you're using anaconda, you'll need to create a folder called Data and point pandas at that folder. If you get stuck, ask a facilitator for help.\n",
    "\n",
    "Once you've read in the data, use the `.head()` method to inspect the beginning of the data. Use `.columns()` to get a list of all the column names. Use `.describe()`, `.unique()`, and `.info()` to do more basic exploration before we begin cleaning the data.\n",
    "\n",
    "##### Remember: you can add new cells by going to insert >> insert cell below. Cell types can be changed under the \"cell\" menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace filename with either a relative or absolute path to the gun violence csv\n",
    "guns = pd.read_csv(filename1)\n",
    "#replace filename with either a relative or absolute path to the victims csv\n",
    "victims = pd.read_csv(filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data\n",
    "It's exceedingly rare that a clean dataset will be available. There are advanced techniques for cleaning and normalizing messy data sets, but the primary goal should be to tidy the data. The two principals of tidy data are as follows:\n",
    "Each column represents a variable.\n",
    "Each row represents an observation.\n",
    "Similar data grouped together is a dataset.\n",
    "\n",
    "The gun-violence.csv file is not a tidy dataset in that each incident has the victims grouped together on the line. This is fine if you want to study the incidents at a macro level, but what if we want to know more about the victims?\n",
    "\n",
    "The task of parsing out each individual victim has been done for you ahead of time. The file victims.csv has the participant data already parsed out. You can merge it back to the original dataset by doing a .merge() on the case_id.\n",
    "\n",
    "### Outliers and Nulls\n",
    "Start by checking on the numerical data such as the victim's age. Are there any missing data? Any outliers? Shown here we see that the oldest person is 311 years old. Does that make sense? What other areas can you check to see where data is missing or not statistically representative? Normalize the data by removing outlying observations with spurious or erronous data. Another technique is to dig deeper into that data point and check it against the source. For example, open the incident_url for the 311 year old victim -- is this a typo or does the incident from the Gun Violence archive support this? If it's from the Gun Violence archive, are there any other errors? Should the whole observation of the incident be removed or just that one individual participant?\n",
    "\n",
    "Work your way through cleaning data using techniques covered in the slides. If in doubt, ask a facilitator for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#who is the oldest victim?\n",
    "victims['participant_age'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show number of nulls per column\n",
    "null_sum = guns.isnull().sum()\n",
    "print(null_sum)\n",
    "\n",
    "# Plot number of nulls per column \n",
    "null_sum.plot(kind='bar')\n",
    "plt.title('Number of null values per column')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cheat sheets to help remove the outlying information. Remember, discussion is key here. When does information become an outlier? Also, consider creating new dataframes containing only information you want to explore further. We'll do more of that in the afternoon of day one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "By now you've already done some EDA work to prep the data and clean it, but let's dive deeper into the data to see what we can find. As you've looked at the data, perhaps some questions have come to mind while you worked. If not, here are some simple questions you can work to find the answer for.\n",
    "\n",
    "* What is the mean, median age of a victim?\n",
    "* Compare the suspects/victim ratio by gender.\n",
    "* Plot shooting incidents over the last year.\n",
    "* What state had the most shootings?\n",
    "* Is there a correlation between guns used and number of victims?\n",
    "\n",
    "### Bringing in New Data\n",
    "\n",
    "As you may have noticed, just using gun violence data alone doesn't give a complete picture. Consider what types of data you might need to answer more advanced data analysis questions. Some examples include:\n",
    "\n",
    "* Census data\n",
    "* Night club, Hospital, Gun Store, etc location information\n",
    "* Gun laws by state\n",
    "* Calendar data\n",
    "* Initimate Partnetr Violence datasets\n",
    "\n",
    "As you explore the data, you may come up with more questions you wish to answer. We'll cover more advanced techniques later, so keep those questions in mind.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show number of incidents per state\n",
    "guns.groupby(['state']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two major ways we can explore this dataset; time series analysis and geospatial analysis. Both can be combined to really drill down and isolate problematic areas. We'll begin by doing time series analysis and then move on to plotting geospatially. Finally, you'll have time to draw your own conclusions through exploration and visualization.\n",
    "\n",
    "## Time Series Analysis\n",
    "\n",
    "As you continue to explore the data, consider questions about the past, present, and future. \n",
    "\n",
    "* When do most shootings take place? Monthly trends? Yearly trends?\n",
    "* What are the trends in shootings?\n",
    "* What day of the week do most shootings occur?\n",
    "\n",
    "Consider what kinds of actions can be done to mitigate deaths based on your time series predictions. What conclusions can we begin to draw? For example, do shootings fall on significant days of the week? Are there any clear outliers in the number of shootings? What happened on those dates? Does the number of incidents tell us something different from what the how deadly a single incident is? What happens if you plot deaths over time versus incidents over time?\n",
    "\n",
    "All of these questions can be answered using time series analysis. As trends are discovered, you can begin to make predictions. For example, notice that July 4th is a very high incident day in America. Why? Is it because it's Independance Day? Are there any other reasons? What kind of datasets would you need to do that type of analysis and can you find them?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one of the first things we need to do is convert the string objects to datetime objects. Thankfully, Pandas and Datetime\n",
    "#work well together\n",
    "\n",
    "# Convert date to pandas datetime object \n",
    "guns['date'] = pd.to_datetime(guns['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What day of the week do most indicents happen?\n",
    "guns['weekday'] = guns['date'].dt.weekday\n",
    "\n",
    "#to do this we'll need to make a new dataframe, so we'll make two series and concat them together\n",
    "days = pd.Series(['Sun', 'Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat'], name = \"day_of_week\")\n",
    "incidents = guns['weekday'].value_counts(sort = False).rename(\"incidents\")\n",
    "inc = pd.concat([days, incidents], axis = 1)\n",
    "#print the dataframe to make sure it works\n",
    "print(inc)\n",
    "\n",
    "#now we can plot the data\n",
    "ax = sns.barplot(x=\"day_of_week\", y=\"incidents\", data=inc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn. Use the cheat sheets and your facilitators, as well as help from the other teams and try to answer questions about the time when incidents occur. Consider merging the victim data in. When are shooting's deadliest? When are they least deadly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geospatial Analysis\n",
    "Just as we can find trends based on time, we can look at the locations where incidents take place. We have location information in the form of states and counties or cities, but also in the form of Latitude and Longitude. We'll walk through using Plotly to visualize state information, and then Folium to drill down on latitude and longitude to discover how trends in shootings map out to regional areas in finer detail. As you work, consider what questions you'd like to answer.\n",
    "\n",
    "* Where do most shootings take place?\n",
    "* What is in the vicinity of the places where shootings take place? What other data can you bring in to get answers?\n",
    "* What's the deadliest county? What's the least deadliest?\n",
    "* What are the characteristics of victims by state?\n",
    "* What are the characteristics of shooters by state?\n",
    "\n",
    "Consider how areas differ geospatially. If you were trying to dispel myths about gun violence, what kind of geospatial data would you need? How can you communicate and inform policy makers and the public about regions where gun violence occurs?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show highest number of incidents by city or county by state \n",
    "guns.groupby('state')[\"city_or_county\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Plotly to Graph State Data\n",
    "\n",
    "This is a fairly simple way to plot geospatial data, and if Folium proves to be difficult to use, Plotly can be used to do simple geospatial analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State Wise Number of Gun Violence Incidents\n",
    "\n",
    "states_df = guns['state'].value_counts()\n",
    "\n",
    "#create a new dataframe and begin adding data to it.\n",
    "statesdf = pd.DataFrame()\n",
    "statesdf['state'] = states_df.index\n",
    "statesdf['counts'] = states_df.values\n",
    "\n",
    "#plot parameters\n",
    "scl = [[0.0, 'rgb(242,240,247)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\\\n",
    "            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']]\n",
    "\n",
    "state_to_code = {'District of Columbia' : 'dc','Mississippi': 'MS', 'Oklahoma': 'OK', 'Delaware': 'DE', \n",
    "                 'Minnesota': 'MN', 'Illinois': 'IL', 'Arkansas': 'AR', 'New Mexico': 'NM', 'Indiana': 'IN', \n",
    "                 'Maryland': 'MD', 'Louisiana': 'LA', 'Idaho': 'ID', 'Wyoming': 'WY', 'Tennessee': 'TN', 'Arizona':'AZ',\n",
    "                 'Iowa': 'IA', 'Michigan': 'MI', 'Kansas': 'KS', 'Utah': 'UT', 'Virginia': 'VA', 'Oregon': 'OR', \n",
    "                 'Connecticut': 'CT', 'Montana': 'MT', 'California': 'CA', 'Massachusetts': 'MA', 'West Virginia': 'WV', \n",
    "                 'South Carolina': 'SC', 'New Hampshire': 'NH', 'Wisconsin': 'WI', 'Vermont': 'VT', 'Georgia': 'GA', \n",
    "                 'North Dakota': 'ND', 'Pennsylvania': 'PA', 'Florida': 'FL', 'Alaska': 'AK', 'Kentucky': 'KY', 'Hawaii': 'HI', \n",
    "                 'Nebraska': 'NE', 'Missouri': 'MO', 'Ohio': 'OH', 'Alabama': 'AL', 'Rhode Island': 'RI', 'South Dakota': 'SD', \n",
    "                 'Colorado': 'CO', 'New Jersey': 'NJ', 'Washington': 'WA', 'North Carolina': 'NC', \n",
    "                 'New York': 'NY', 'Texas': 'TX', 'Nevada': 'NV', 'Maine': 'ME'}\n",
    "statesdf['state_code'] = statesdf['state'].apply(lambda x : state_to_code[x])\n",
    "\n",
    "#data to be passed in by plotly\n",
    "data = [ dict(\n",
    "        type='choropleth',\n",
    "        colorscale = scl,\n",
    "        autocolorscale = False,\n",
    "        locations = statesdf['state_code'],\n",
    "        z = statesdf['counts'],\n",
    "        locationmode = 'USA-states',\n",
    "        text = statesdf['state'],\n",
    "        marker = dict(\n",
    "            line = dict (\n",
    "                color = 'rgb(255,255,255)',\n",
    "                width = 2\n",
    "            ) ),\n",
    "        colorbar = dict(\n",
    "            title = \"Gun Violence Incidents\")\n",
    "        ) ]\n",
    "\n",
    "layout = dict(\n",
    "        title = 'State wise number of Gun Violence Incidents',\n",
    "        geo = dict(\n",
    "            scope='usa',\n",
    "            projection=dict( type='albers usa' ),)\n",
    "             )\n",
    "#and plot time!    \n",
    "fig = dict( data=data, layout=layout )\n",
    "iplot( fig, filename='d3-cloropleth-map.html' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Folium to See Individual Incidents\n",
    "\n",
    "If you've installed Folium, you can use it to create pointers on a map for each incident, as well as cluster data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse data set for plotting\n",
    "df_gun_map = guns[['date','state','latitude', 'longitude']].copy()\n",
    "# Drop any rows with NaN Values\n",
    "df_gun_map.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate a single state -- Folium can only handle a couple hundred markers at a time, so find ways to segregate data.\n",
    "state = 'Rhode Island'\n",
    "df_gun_map_RI = df_gun_map[df_gun_map[\"state\"] == state].copy()\n",
    "#get rid of the date and state columns\n",
    "df_gun_map_RI.drop(['date','state'], axis=1, inplace=True)\n",
    "\n",
    "# Convert dataframe of coordinates to list for Folium\n",
    "locationlist = df_gun_map_RI.values.tolist()\n",
    "# Build base map and show it\n",
    "gun_map = folium.Map(location=locationslist[0], zoom_start= 7)\n",
    "gun_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add locations for the points\n",
    "for point in range(0, len(locationlist)):\n",
    "    folium.Marker(locationlist[point]).add_to(gun_map)\n",
    "gun_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize clusters\n",
    "marker_cluster = MarkerCluster().add_to(gun_map)\n",
    "\n",
    "# Display locations in clusters\n",
    "for point in range(0, len(locationlist)):\n",
    "    folium.Marker(locationlist[point]).add_to(marker_cluster)\n",
    "gun_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the basics of folium down, how can you use both Geospatial and Time Series analysis to drill down and find trends?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn:\n",
    "\n",
    "Now that you've walked through some of getting started with time series and geospatial analysis, spend time exploring the data and doing your own analysis. Consider what questions you'd like answered and begin answering them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Machine Learning and Predictive Analysis\n",
    "\n",
    "Where will the next shooting happen? When should hospitals be staffed for shootings? When should police and emergency responders be on alert for incidents? Who needs intervention to minimize gun violence? How will these trends continue into the future?\n",
    "\n",
    "All these are questions that plague people working to save lives and end the gun violence epidemic. Machine learning is a technique that uses the past to guess at what the future will hold. We can apply it here in a variety of ways. There are two main types of machine learning: supervised and unsupervised. We'll work primarily with supervised machine learning for this dataset. Beyond that, there are several main tasks for the machines: categorizing data based on it's features or predicting out the future.\n",
    "\n",
    "![Python's ML Package, Scikit-learn has it's own cheat sheet](http://scikit-learn.org/stable/_static/ml_map.png)\n",
    "http://scikit-learn.org/stable/_static/ml_map.png\n",
    "\n",
    "You may have seen while doing your exploratory analysis that some trends are clearer than others. There's always a fuzzy area and machine learnining works to handle that ambiguity in ways that humans are not so great at. Of course, there's plenty of legal and ethical questions surrounding the use of machine learning, as input biases lead to output biases and creating fairness in artificial intelligence is a hot topic, but for now we'll use machines to see if we can draw any new insights from out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Up\n",
    "\n",
    "What did you find during this exercise? What insights would you like to share with the world from doing this exploration? Here is your chance to make conclusions and present your findings. When you're finished, you may submit your kernel/notebook to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
